{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d1af08d-bc91-4b66-a87b-4b2cd2cc40e5",
   "metadata": {},
   "source": [
    "# Dataset 1: Synthetic Circle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbdf65c-f5bd-4c32-af1e-749c587e4ebe",
   "metadata": {},
   "source": [
    "## Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a2d46a-ca99-49d6-8f5a-33d6d7c86894",
   "metadata": {},
   "source": [
    "The **Synthetic Circle dataset** is an artificial 2D dataset containing **10,000 points** grouped into **100 circular clusters**, with each cluster containing **100 points**.  \n",
    "It includes two real-valued features, x and y, representing the coordinates of each point, and a class label identifying which circle each point belongs to. The dataset is designed to test clustering algorithms.  \n",
    "\n",
    "Here, **“conflicting”** refers to the **mismatch between the circular (non-convex) structure of the data and the assumptions of traditional clustering methods** like K-Means, which expect clusters to be roughly spherical and convex. Thus, while the clusters are clearly defined visually, algorithms relying solely on Euclidean distance may struggle to capture their true shape.\n",
    "\n",
    "We shall use **K-means** and **DBSCAN** for clustering and compare the results using the **Davies–Bouldin index**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d178121d-f1e6-4902-8147-5336f510c570",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41d5186-2449-4588-939a-20eedf901943",
   "metadata": {},
   "source": [
    "### Reading and Data Preprocessing\n",
    "\n",
    "In this section, we will take a look at the structure of the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d46ac10-a88b-4f5f-a569-dbf1196aa086",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2f0ab8-3846-4ab8-b527-c4a4907d75c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"circles.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cda0cf-2854-4344-82b9-3232b065416b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a0716d-0cd5-42d9-9120-649f08efafea",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dd4121-f639-4763-9897-c93aee1dde16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e8a399-8def-462e-bcb0-84381064ec74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8770921-6188-43b6-bde1-f6f6a83ed97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e44a941-6d55-467e-b1eb-8037535f733c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c55e8899-9498-4ee3-bf4d-2efd75abe455",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8fe6dd-4186-445e-ad9e-58f111e5196b",
   "metadata": {},
   "source": [
    "So upon looking at the data, we see that there are no missing values, and all values are of the float type. Additionally, there are no duplicates in the dataset. We will now move on to the next part - Data Visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fee291c-7dd1-4f2f-bc93-2fc12bb48b31",
   "metadata": {},
   "source": [
    "### Data Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ac957f4-d325-4aa9-b400-f1bf6b6d80d0",
   "metadata": {},
   "source": [
    "In this section, we aim to visually explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c80b6a3-3161-439a-88de-fec04ebe4f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(df[\"x\"], df[\"y\"], s = 10, alpha = 0.5)\n",
    "plt.title(\"Scatter Plot of Synthetic Circle Dataset\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c945e77-4465-4987-a548-7d3d856493ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairplot = sns.pairplot(df, hue='class')\n",
    "pairplot.fig.suptitle(\"Pair Plot of Synthetic Circle Dataset\", y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb8ffab-3c0e-4dce-a570-d7a34a097f9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_df = df.select_dtypes(include = [np.number])\n",
    "correlation_matrix = number_df.corr()\n",
    "plt.figure(figsize=(5,5))\n",
    "sns.heatmap(correlation_matrix, annot = True)\n",
    "plt.title(\"Correlation Matrix of Synthetic Circle Dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f52f8e-9540-49fd-b00b-f03d15492ff6",
   "metadata": {},
   "source": [
    "#### Inferences from the plots:\n",
    "\n",
    "1) The scatter plot of the dataset reveals a highly structured and symmetric spatial pattern. Each of the 100 clusters forms a distinct circular ring arranged in a uniform grid. This confirms that the data is clean, evenly distributed, and perfectly separated, with no missing or overlapping points. \n",
    "\n",
    "2) The pair plot shows the pairwise relationships between x, y, and class.  Both the x and y features exhibit periodic and repeating distributions, which is consistent with the circular arrangement of the points.  \n",
    "\n",
    "3) Finally, the correlation heatmap shows that x and y are nearly uncorrelated, which means there is no linear dependency between the two coordinates.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3e43d9-01f5-48f4-9357-ef2d64e7b81e",
   "metadata": {},
   "source": [
    "## Clustering Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9d0e58-e59d-4549-9017-582f9fa7cca1",
   "metadata": {},
   "source": [
    "### K-Means Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13757ba9-f8d5-457a-92e6-4c073dbf8544",
   "metadata": {},
   "source": [
    "K-Means partitions data into *k* clusters by minimizing the distance between points and their cluster centroids.  \n",
    "It iteratively:\n",
    "1. Initializes *k* random centroids.  \n",
    "2. Assigns each point to the nearest centroid.  \n",
    "3. Updates each centroid as the mean of its assigned points.  \n",
    "The process repeats until centroids stop changing (convergence).\n",
    "\n",
    "**Hyperparameters:**\n",
    "1) The number of clusters k = 100 was chosen since the dataset is known to contain 100 distinct circular clusters.\n",
    "2) The distance metric used is the Euclidean distance, which measures the straight-line distance between points and centroids.\n",
    "3) Convergence threshold epsilon = 1e−4 ensures the algorithm stops only when centroid movement is minimal, and a maximum iteration limit (max_iter = 500) prevents infinite loops.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c460c7-a481-4034-9663-416f7ae5cc18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class KMeansClustering:\n",
    "    def __init__(self, k, epsilon = 1e-9, max_iter = 2000):\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        self.max_iter = max_iter\n",
    "        self.centroids = []\n",
    "        self.clusters = []\n",
    "\n",
    "    def cluster_points(self, points):\n",
    "        \"\"\" \n",
    "        Measure distance to each centroid and assign points to the nearest cluster \n",
    "        \"\"\"\n",
    "        self.clusters = []\n",
    "        for i in range(self.k):\n",
    "            self.clusters.append([])\n",
    "        for point in points:\n",
    "            pt_distances = [np.linalg.norm(point - self.centroids[i], axis=0) for i in range(self.k)]\n",
    "            closest_cluster_index = int(np.argmin(pt_distances))\n",
    "            self.clusters[closest_cluster_index].append(point)\n",
    "\n",
    "    def recalculate_centroid(self):\n",
    "        \"\"\"\n",
    "        recalculate centroid to be the mean of all of its cluster's points\n",
    "        \"\"\"\n",
    "        for i in range(self.k):\n",
    "            if len(self.clusters[i]) > 0:\n",
    "                self.centroids[i] = np.mean(self.clusters[i], axis=0)\n",
    "            else:\n",
    "                self.centroids[i] = self.centroids[i]\n",
    "\n",
    "    def convergence(self, old_centroids):\n",
    "        \"\"\"\n",
    "        Checks convergence\n",
    "        \"\"\"\n",
    "        all_distances = np.linalg.norm(np.array(old_centroids) - np.array(self.centroids), axis=1)\n",
    "        max_dist = all_distances.max()\n",
    "        return max_dist < self.epsilon\n",
    "\n",
    "    def fit(self, points):\n",
    "        \"\"\"\n",
    "        training k-means model\n",
    "        \"\"\"\n",
    "        points = np.array(points)\n",
    "        self.centroids = random.sample(list(points), self.k)\n",
    "        for i in range(self.max_iter):\n",
    "            old_centroids = self.centroids.copy()\n",
    "            self.cluster_points(points)\n",
    "            self.recalculate_centroid()\n",
    "            if self.convergence(old_centroids):\n",
    "                print(f\"Converged at {i+1} iteration\")\n",
    "                break\n",
    "        return self.clusters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eff5e3c-0f3f-4a64-8550-5f488046f8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"x\", \"y\"]].to_numpy()\n",
    "\n",
    "kmeans = KMeansClustering(k=100, epsilon = 1e-4, max_iter = 500)\n",
    "clusters = kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb581adf-1eb0-4e6d-bb2a-9c98eed971ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7, 7))\n",
    "for cluster in clusters:\n",
    "    cluster = np.array(cluster)\n",
    "    if len(cluster):\n",
    "        plt.scatter(cluster[:,0], cluster[:,-1], s =6, alpha = 0.7)\n",
    "centroids = np.array(kmeans.centroids)\n",
    "plt.scatter(centroids[:,0], centroids[:,1], c='k', s=60, marker='x')\n",
    "plt.title('K-Means')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0366837-7b32-4f08-95fd-4d6c98a0f0f6",
   "metadata": {},
   "source": [
    "Now that we have our clustering, let us evaluate this with Davies–Bouldin Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c3eaa4-a83a-4736-a5a5-6acd963195c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = np.array(kmeans.centroids)\n",
    "\n",
    "def assign_labels_from_centroids(X, centroids):\n",
    "    d = np.linalg.norm(X[:, None, :] - centroids[None, :, :], axis=2)  # (n,k)\n",
    "    return np.argmin(d, axis=1)\n",
    "\n",
    "labels = assign_labels_from_centroids(X, centroids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c193866a-a637-46e6-a549-085e382717b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def davies_bouldin_index(X, labels, centroids, eps=1e-12):\n",
    "    K = len(centroids)\n",
    "    S = np.zeros(K, dtype=float)\n",
    "    for k in range(K):\n",
    "        pts = X[labels == k]\n",
    "        if len(pts) == 0:\n",
    "            S[k] = 0.0\n",
    "        else:\n",
    "            S[k] = np.mean(np.linalg.norm(pts - centroids[k], axis=1))\n",
    "\n",
    "    # centroid distances M_{ij}\n",
    "    M = np.linalg.norm(centroids[:, None, :] - centroids[None, :, :], axis=2) + eps\n",
    "\n",
    "    # R_{ij} = (S_i + S_j) / M_{ij}; for i!=j\n",
    "    R = (S[:, None] + S[None, :]) / M\n",
    "    np.fill_diagonal(R, -np.inf)  # ignore i=j\n",
    "\n",
    "    D = np.max(R, axis=1)\n",
    "    return float(np.mean(D))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3abcc36-dbc7-4111-a164-4c071509841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbi = davies_bouldin_index(X, labels, centroids)\n",
    "print(\"DBI:\", round(dbi, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914d9d6b-126e-466d-a760-4b2313d63365",
   "metadata": {},
   "source": [
    "### DBSCAN Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88604142-6df0-4c59-bdd0-81c7337a5357",
   "metadata": {},
   "source": [
    "DBSCAN clusters points based on density.  \n",
    "\n",
    "**Algorithm :**\n",
    "1) For each point, find all neighboring points within distance epsilon (eps).  \n",
    "2) Mark points with at least MinPts neighbors as core points.  \n",
    "3) For each unvisited core point, form a new cluster and recursively include all points density-connected to it.  \n",
    "4) Points that are not part of any cluster are labeled as noise.\n",
    "\n",
    "**Hyperparameters:** \n",
    "1) eps: neighborhood radius  = 2\n",
    "2) MinPts: minimum neighbors to form a cluster = 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33e48d7-006c-4194-9d08-eb19a965e16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "\n",
    "class DBSCAN:\n",
    "    def __init__(self, eps, min_pts):\n",
    "        self.eps = float(eps)\n",
    "        self.min_pts = int(min_pts)\n",
    "        self.core_sample_mask_ = None\n",
    "        self.n_clusters_ = 0\n",
    "        self.labels_ = None\n",
    "        \n",
    "    def _region_query(self, points, i):\n",
    "        neighbours = []\n",
    "        for j in range(len(points)):\n",
    "            if np.linalg.norm(points[i] - points[j]) <= self.eps:\n",
    "                neighbours.append(j)\n",
    "        return np.array(neighbours)\n",
    "\n",
    "    def fit(self, points):\n",
    "        points = np.array(points)\n",
    "        N = len(points)\n",
    "        self.labels_ = -np.ones(N, dtype=int)\n",
    "        visited = np.zeros(N, dtype=bool)\n",
    "        self.core_sample_mask_ = np.zeros(N, dtype=bool)\n",
    "\n",
    "        neighborhoods = [self._region_query(points, i) for i in range(N)]\n",
    "\n",
    "        for i in range(N):\n",
    "            if len(neighborhoods[i]) >= self.min_pts:\n",
    "                self.core_sample_mask_[i] = True\n",
    "\n",
    "        cluster_id = 0\n",
    "        for i in range(N):\n",
    "            if visited[i]:\n",
    "                continue\n",
    "            visited[i] = True\n",
    "\n",
    "            if not self.core_sample_mask_[i]:\n",
    "                continue\n",
    "\n",
    "            self.labels_[i] = cluster_id\n",
    "            queue = deque([i])\n",
    "\n",
    "            while queue:\n",
    "                j = queue.popleft()\n",
    "                if not self.core_sample_mask_[j]:\n",
    "                    continue\n",
    "                for k in neighborhoods[j]:\n",
    "                    if not visited[k]:\n",
    "                        visited[k] = True\n",
    "                        if self.core_sample_mask_[k]:\n",
    "                            queue.append(k)\n",
    "                    if self.labels_[k] == -1:\n",
    "                        self.labels_[k] = cluster_id\n",
    "            cluster_id += 1\n",
    "\n",
    "        self.n_clusters_ = cluster_id\n",
    "        return self\n",
    "\n",
    "    def fit_predict(self, X):\n",
    "        return self.fit(X).labels_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2469f6-c6ef-4b01-beca-7584fbaf8943",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"x\", \"y\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eac38f-44b5-4d29-9bc4-26e51934f782",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=2, min_pts=5)\n",
    "labels = db.fit_predict(X)\n",
    "\n",
    "print(\"Clusters:\", db.n_clusters_)\n",
    "print(\"Noise points:\", (labels == -1).sum())\n",
    "print(\"Per-cluster counts:\", {c:int((labels==c).sum()) for c in range(db.n_clusters_)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533b522e-589f-469f-94a3-eaba40eb8f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 2.0\n",
    "min_pts = 5\n",
    "if X.shape[1] == 2:\n",
    "    unique = sorted(set(labels))\n",
    "    for lab in unique:\n",
    "        mask = labels == lab\n",
    "        plt.scatter(X[mask,0], X[mask,1], s=12, label=(\"noise\" if lab==-1 else f\"C{lab}\"))\n",
    "    plt.title(f\"DBSCAN eps={eps:.3f}, MinPts={min_pts}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3732513f-8c34-4d69-b516-b26124c82c55",
   "metadata": {},
   "source": [
    "Now that we have our clustering, let us evaluate this with Davies–Bouldin Index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcafa0b-86c3-40e6-b0c3-efb345903a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def davies_bouldin_index(X, labels, centroids=None, ignore_noise=True, eps=1e-12):\n",
    "    X = np.asarray(X)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    mask = labels != -1 if ignore_noise else np.ones_like(labels, dtype=bool)\n",
    "    Xv, Lv = X[mask], labels[mask]\n",
    "\n",
    "    unique = np.unique(Lv)\n",
    "    K = unique.size\n",
    "    if K < 2:\n",
    "        return np.nan \n",
    "\n",
    "    lab2idx = {lab: i for i, lab in enumerate(unique)}\n",
    "    idx = np.array([lab2idx[lab] for lab in Lv])\n",
    "\n",
    "    if centroids is None:\n",
    "        centroids = np.vstack([Xv[idx == i].mean(axis=0) for i in range(K)])\n",
    "    else:\n",
    "        centroids = np.asarray(centroids)\n",
    "        assert centroids.shape[0] == K, \"centroids count must match #clusters\"\n",
    "\n",
    "    S = np.zeros(K, dtype=float)\n",
    "    for i in range(K):\n",
    "        Xi = Xv[idx == i]\n",
    "        S[i] = 0.0 if Xi.shape[0] == 0 else np.linalg.norm(Xi - centroids[i], axis=1).mean()\n",
    "\n",
    "    diff = centroids[:, None, :] - centroids[None, :, :]\n",
    "    M = np.linalg.norm(diff, axis=2)\n",
    "    np.fill_diagonal(M, np.inf)  # exclude i=j\n",
    "\n",
    "    \n",
    "    R = (S[:, None] + S[None, :]) / (M + eps)\n",
    "    np.fill_diagonal(R, -np.inf)\n",
    "    return float(R.max(axis=1).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2884b2-23cb-48b3-8048-a0a5139cb15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = db.labels_\n",
    "dbi = davies_bouldin_index(X, labels, ignore_noise=True)\n",
    "print(\"DBI:\", dbi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d1a738-19e3-48d5-8f86-090937958c07",
   "metadata": {},
   "source": [
    "### Comparision of the Methods:\n",
    "\n",
    "#### **K-Means**\n",
    "**Pros:**\n",
    "1) Simple, fast, and efficient for large, spherical datasets.  \n",
    "2) Guarantees convergence and works well when clusters are well-separated and similar in size.\n",
    "\n",
    "**Cons:**\n",
    "1) Requires the number of clusters (*k*) in advance.   \n",
    "2) Assumes clusters are isotropic (spherical), which conflicts with circular data.\n",
    "\n",
    "**Use Case:**  \n",
    "Best for compact, evenly sized clusters where the number of clusters is known.\n",
    "\n",
    "**Evaluation:**  \n",
    "K-Means achieved a **DBI of 0.694**, indicating reasonably compact clusters but some mismatch due to the ring-shaped structure.\n",
    "\n",
    "#### **DBSCAN**\n",
    "**Pros:**\n",
    "1) Does not require specifying the number of clusters.\n",
    "2) Can find clusters of arbitrary shapes and identify noise points automatically.  \n",
    "3) Robust to outliers.\n",
    "\n",
    "**Cons:**\n",
    "1) Sensitive to choice of epsilon and MinPts parameters.  \n",
    "\n",
    "**Use Case:**  \n",
    "Ideal for datasets with unknown number of clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04bd488-88be-44dc-a00c-5cdcf0f5fa22",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae26bc79-8f18-4ac5-9881-ce52649ac77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(df[['x', 'y']])\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=2)   # You can use 1 or 2 to visualize explained variance\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Create a new DataFrame with PCA results\n",
    "pca_df = pd.DataFrame(data=X_pca, columns=['PC1', 'PC2'])\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "print(\"Total Variance Explained:\", np.sum(pca.explained_variance_ratio_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0fb2e0-6fc7-46e4-97bc-dbdd32b2690a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(pca_df['PC1'], pca_df['PC2'], s=10, alpha=0.6)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA Projection of Synthetic Ring Data')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75dd217a-7ef9-4fb5-883b-9fab70e0f60c",
   "metadata": {},
   "source": [
    "### Choice of Dimensionality Reduction Method\n",
    "\n",
    "Principal Component Analysis (PCA) was chosen because it is a simple and widely used linear technique for reducing dimensionality . It preserves the maximum possible variance in the data.  It transforms correlated features into uncorrelated principal components, making it easier to visualize data and reduce redundancy.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5714957b-b615-42ea-8d78-94bd85202c31",
   "metadata": {},
   "source": [
    "### Retrying K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630156b7-7542-45e6-86e0-78d2a5c16021",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"x\", \"y\"]].to_numpy()\n",
    "\n",
    "kmeans = KMeansClustering(k=100, epsilon = 1e-4, max_iter = 500)\n",
    "clusters = kmeans.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d320ca78-1903-47ae-96ee-4a0fae547974",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (7, 7))\n",
    "for cluster in clusters:\n",
    "    cluster = np.array(cluster)\n",
    "    if len(cluster):\n",
    "        plt.scatter(cluster[:,0], cluster[:,-1], s =6, alpha = 0.7)\n",
    "centroids = np.array(kmeans.centroids)\n",
    "plt.scatter(centroids[:,0], centroids[:,1], c='k', s=60, marker='x')\n",
    "plt.title('K-Means')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee4d664-fda6-4732-876a-b3afd0f81574",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids = np.array(kmeans.centroids)\n",
    "\n",
    "def assign_labels_from_centroids(X, centroids):\n",
    "    d = np.linalg.norm(X[:, None, :] - centroids[None, :, :], axis=2)  # (n,k)\n",
    "    return np.argmin(d, axis=1)\n",
    "\n",
    "labels = assign_labels_from_centroids(X, centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe012f1-d09f-4788-9afe-d5b76670b4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def davies_bouldin_index(X, labels, centroids, eps=1e-12):\n",
    "    K = len(centroids)\n",
    "    S = np.zeros(K, dtype=float)\n",
    "    for k in range(K):\n",
    "        pts = X[labels == k]\n",
    "        if len(pts) == 0:\n",
    "            S[k] = 0.0\n",
    "        else:\n",
    "            S[k] = np.mean(np.linalg.norm(pts - centroids[k], axis=1))\n",
    "\n",
    "    # centroid distances M_{ij}\n",
    "    M = np.linalg.norm(centroids[:, None, :] - centroids[None, :, :], axis=2) + eps\n",
    "\n",
    "    # R_{ij} = (S_i + S_j) / M_{ij}; for i!=j\n",
    "    R = (S[:, None] + S[None, :]) / M\n",
    "    np.fill_diagonal(R, -np.inf)  # ignore i=j\n",
    "\n",
    "    D = np.max(R, axis=1)\n",
    "    return float(np.mean(D))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05080283-a7b2-42ce-a186-df46756b652a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbi = davies_bouldin_index(X, labels, centroids)\n",
    "print(\"DBI:\", round(dbi, 6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a6cb2c-cd73-47f1-9c60-23cc20eafc25",
   "metadata": {},
   "source": [
    "### Retrying DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28243656-f81a-4c0a-bdc5-f102f7459a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"x\", \"y\"]].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b5b6e3-fdf6-47d5-bfc9-7875ef02fa2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = DBSCAN(eps=2, min_pts=5)\n",
    "labels = db.fit_predict(X)\n",
    "\n",
    "print(\"Clusters:\", db.n_clusters_)\n",
    "print(\"Noise points:\", (labels == -1).sum())\n",
    "print(\"Per-cluster counts:\", {c:int((labels==c).sum()) for c in range(db.n_clusters_)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a25714-4c94-4ac4-b85f-9e74a18d5de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 2.0\n",
    "min_pts = 5\n",
    "if X.shape[1] == 2:\n",
    "    unique = sorted(set(labels))\n",
    "    for lab in unique:\n",
    "        mask = labels == lab\n",
    "        plt.scatter(X[mask,0], X[mask,1], s=12, label=(\"noise\" if lab==-1 else f\"C{lab}\"))\n",
    "    plt.title(f\"DBSCAN eps={eps:.3f}, MinPts={min_pts}\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764bdafd-47bb-4f86-97fc-e0fa55938b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def davies_bouldin_index(X, labels, centroids=None, ignore_noise=True, eps=1e-12):\n",
    "    X = np.asarray(X)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    mask = labels != -1 if ignore_noise else np.ones_like(labels, dtype=bool)\n",
    "    Xv, Lv = X[mask], labels[mask]\n",
    "\n",
    "    unique = np.unique(Lv)\n",
    "    K = unique.size\n",
    "    if K < 2:\n",
    "        return np.nan \n",
    "\n",
    "    lab2idx = {lab: i for i, lab in enumerate(unique)}\n",
    "    idx = np.array([lab2idx[lab] for lab in Lv])\n",
    "\n",
    "    if centroids is None:\n",
    "        centroids = np.vstack([Xv[idx == i].mean(axis=0) for i in range(K)])\n",
    "    else:\n",
    "        centroids = np.asarray(centroids)\n",
    "        assert centroids.shape[0] == K, \"centroids count must match #clusters\"\n",
    "\n",
    "    S = np.zeros(K, dtype=float)\n",
    "    for i in range(K):\n",
    "        Xi = Xv[idx == i]\n",
    "        S[i] = 0.0 if Xi.shape[0] == 0 else np.linalg.norm(Xi - centroids[i], axis=1).mean()\n",
    "\n",
    "    diff = centroids[:, None, :] - centroids[None, :, :]\n",
    "    M = np.linalg.norm(diff, axis=2)\n",
    "    np.fill_diagonal(M, np.inf)  # exclude i=j\n",
    "\n",
    "    \n",
    "    R = (S[:, None] + S[None, :]) / (M + eps)\n",
    "    np.fill_diagonal(R, -np.inf)\n",
    "    return float(R.max(axis=1).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c279b96f-f072-433c-90fc-7e5c3f14abcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = db.labels_\n",
    "dbi = davies_bouldin_index(X, labels, ignore_noise=True)\n",
    "print(\"DBI:\", dbi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052897f9-7bb1-4347-b33f-75d24cde9cc7",
   "metadata": {},
   "source": [
    "### Before and After Dimensionality Reduction\n",
    "\n",
    "After applying PCA to the dataset, there was no significant change in the clustering results for either K-Means or DBSCAN. Since the data originally had only two features (x and y), PCA did not reduce dimensionality or alter the structure of the dataset. The visual structure of the circular clusters was preserved.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e59a703-ebcd-499e-b2fe-d1525b89fa57",
   "metadata": {},
   "source": [
    "## Comparison & Discussion\n",
    "\n",
    "**Best Method:**  \n",
    "DBSCAN performed better than K-Means, achieving a lower DBI (0.511 vs. 0.694). It handled the circular, non-convex clusters more effectively and identified dense regions accurately.\n",
    "\n",
    "**Effect of Dimensionality Reduction:**  \n",
    "PCA had no effect on clustering since the dataset is already two-dimensional. Both algorithms produced identical results before and after reduction.\n",
    "\n",
    "**Limitations:**  \n",
    "1) K-Means assumes spherical clusters and can struggle with non-linear structures.  \n",
    "2) DBSCAN’s performance depends heavily on epsilon and MinPts selection.  \n",
    "3) No noise or overlapping clusters in the dataset limits testing of robustness.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13993965-2e3f-4503-871b-68db9cedb185",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This exercise demonstrated how different clustering algorithms perform on the Synthetic Circle dataset.  K-Means successfully separated the clusters but DBSCAN also accurately detected the ring-shaped structures and achieved a lower DBI, indicating better cluster quality. Dimensionality reduction through PCA had no effect since the data was already two-dimensional.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e533597",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook performs clustering analysis on the Portuguese \n",
    "Bank Marketing dataset using custom implementations of K-Means and K-Prototypes\n",
    "algorithms, with and without dimensionality reduction for my IML assignment.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial.distance import cdist\n",
    "from scipy import stats\n",
    "from collections import deque\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.size'] = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456cfdda",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "## 1.1 Dataset Overview\n",
    "\n",
    "The **Bank Marketing dataset** originates from the direct marketing campaigns (phone calls) of a Portuguese banking institution. The marketing campaigns were based on phone calls, and often, more than one contact to the same client was required to assess if the product (bank term deposit) would be subscribed to or not.\n",
    "\n",
    "**Dataset Characteristics:**\n",
    "- **Type:** Multivariate  \n",
    "- **Number of Instances:** 45,211  \n",
    "- **Number of Features:** 17 (16 predictors + 1 target variable)  \n",
    "- **Feature Types:** Categorical and Integer  \n",
    "- **Target Variable:** Binary (`yes`/`no`) - indicates whether the client subscribed to a term deposit  \n",
    "- **Missing Values:** None  \n",
    "- **Duplicates:** None  \n",
    "\n",
    "---\n",
    "\n",
    "## 1.2 Feature Descriptions\n",
    "\n",
    "The dataset contains 17 attributes grouped into four categories:\n",
    "\n",
    "### A. Client Demographics (8 features)\n",
    "\n",
    "1. **age** (numeric): Age of the client (18–95 years)  \n",
    "2. **job** (categorical): Type of occupation - admin, blue-collar, entrepreneur, housemaid, management, retired, self-employed, services, student, technician, unemployed, unknown  \n",
    "3. **marital** (categorical): Marital status - divorced, married, single  \n",
    "4. **education** (categorical): Education level - primary, secondary, tertiary, unknown  \n",
    "5. **default** (categorical): Has credit in default? (`yes`/`no`)  \n",
    "6. **balance** (numeric): Average yearly balance in euros (−8019 to 102127)  \n",
    "7. **housing** (categorical): Has housing loan? (`yes`/`no`)  \n",
    "8. **loan** (categorical): Has personal loan? (`yes`/`no`)  \n",
    "\n",
    "---\n",
    "\n",
    "### B. Campaign Contact Information (4 features)\n",
    "\n",
    "9. **contact** (categorical): Contact communication type - cellular, telephone, unknown  \n",
    "10. **day** (numeric): Last contact day of the month (1–31)  \n",
    "11. **month** (categorical): Last contact month - jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec  \n",
    "12. **duration** (numeric): Last contact duration in seconds (0–4918)  \n",
    "\n",
    "---\n",
    "\n",
    "### C. Campaign History (3 features)\n",
    "\n",
    "13. **campaign** (numeric): Number of contacts during this campaign (1–63)  \n",
    "14. **pdays** (numeric): Days since last contact from previous campaign (−1 to 871)  \n",
    "   - *−1 indicates never previously contacted (81.7% of clients)*  \n",
    "15. **previous** (numeric): Number of contacts before this campaign (0–275)  \n",
    "16. **poutcome** (categorical): Outcome of previous marketing campaign - failure, other, success, unknown  \n",
    "\n",
    "---\n",
    "\n",
    "### D. Target Variable\n",
    "\n",
    "17. **y** (categorical): Has the client subscribed to a term deposit?  \n",
    "   - Categories: `yes` (11.7%), `no` (88.3%)  \n",
    "\n",
    "---\n",
    "\n",
    "## 1.3 Objectives of This Notebook\n",
    "\n",
    "**Primary Objectives:**\n",
    "1. Implement custom clustering algorithms:  \n",
    "   - K-Means (from scratch)  \n",
    "   - DBSCAN (from scratch)\n",
    "2. Explore dimensionality reduction:  \n",
    "   - Apply PCA  \n",
    "   - Compare clustering before and after reduction  \n",
    "3. Evaluate clustering quality:  \n",
    "   - Use **Silhouette Score**  \n",
    "4. Compare clustering methods:  \n",
    "   - Analyze **K-Means vs K-Prototypes** performance  \n",
    "\n",
    "**Secondary Objectives:**\n",
    "- Conduct exploratory data analysis (EDA)  \n",
    "- Perform preprocessing and feature picking  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacd284a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bank = pd.read_csv(\"bank-full.csv\", sep=';')\n",
    "\n",
    "\n",
    "print(\"BANK MARKETING DATASET - INITIAL INSPECTION\")\n",
    "\n",
    "print(f\"\\nDataset Shape: {df_bank.shape[0]:,} samples × {df_bank.shape[1]} features\")\n",
    "print(\"\\nFirst 5 Rows:\")\n",
    "display(df_bank.head())\n",
    "\n",
    "print(\"\\nDataset Information:\")\n",
    "df_bank.info()\n",
    "\n",
    "print(\"\\nBasic Statistics:\")\n",
    "display(df_bank.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4338965d",
   "metadata": {},
   "source": [
    "## 2.2 Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0960f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "\n",
    "\n",
    "print(\"\\nMissing Values Check:\")\n",
    "missing_vals = df_bank.isnull().sum()\n",
    "if missing_vals.sum() == 0:\n",
    "    print(\"No missing values detected in any feature\")\n",
    "else:\n",
    "    display(missing_vals[missing_vals > 0])\n",
    "\n",
    "duplicates = df_bank.duplicated().sum()\n",
    "print(f\"\\nDuplicate Rows: {duplicates}\")\n",
    "if duplicates == 0:\n",
    "    print(\"No duplicate records found\")\n",
    "\n",
    "print(\"\\nCategorical Features with 'unknown' Values:\")\n",
    "categorical_cols = df_bank.select_dtypes(include=['object']).columns\n",
    "unknown_summary = []\n",
    "\n",
    "for col in categorical_cols:\n",
    "    unknown_count = (df_bank[col] == 'unknown').sum()\n",
    "    unknown_pct = 100 * unknown_count / len(df_bank)\n",
    "    if unknown_count > 0:\n",
    "        unknown_summary.append({\n",
    "            'Feature': col,\n",
    "            'Unknown Count': unknown_count,\n",
    "            'Unknown %': f'{unknown_pct:.2f}%'\n",
    "        })\n",
    "\n",
    "if unknown_summary:\n",
    "    unknown_df = pd.DataFrame(unknown_summary)\n",
    "    display(unknown_df)\n",
    "    print(\"\\nNote: 'unknown' values are valid categories, not missing data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf100add",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TARGET VARIABLE ANALYSIS\")\n",
    "\n",
    "\n",
    "target_counts = df_bank['y'].value_counts()\n",
    "target_props = df_bank['y'].value_counts(normalize=True)\n",
    "\n",
    "print(\"\\nSubscription to Term Deposit (Target Variable 'y'):\")\n",
    "print(f\"  No:  {target_counts['no']:>6,} samples ({target_props['no']:.2%})\")\n",
    "print(f\"  Yes: {target_counts['yes']:>6,} samples ({target_props['yes']:.2%})\")\n",
    "print(f\"\\nClass Imbalance Ratio: {target_counts['no']/target_counts['yes']:.2f}:1\")\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"  • Highly imbalanced dataset with 88.3% negative class\")\n",
    "print(\"  • Only 11.7% of clients subscribed to term deposit\")\n",
    "print(\"  • This reflects real-world campaign success rates\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# Bar plot \n",
    "colors = ['#e74c3c', '#2ecc71']\n",
    "bars = axes[0].bar(target_counts.index, target_counts.values, color=colors, \n",
    "                   alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "axes[0].set_title('Target Variable Distribution\\n(Absolute Counts)', \n",
    "                  fontweight='bold', fontsize=12)\n",
    "axes[0].set_ylabel('Number of Clients', fontweight='bold')\n",
    "axes[0].set_xlabel('Subscribed to Term Deposit', fontweight='bold')\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "for i, (idx, val) in enumerate(target_counts.items()):\n",
    "    axes[0].text(i, val + 800, f'{val:,}\\n({target_props[idx]:.1%})', \n",
    "                ha='center', fontweight='bold', fontsize=10)\n",
    "\n",
    "# Pie chart\n",
    "explode = (0.05, 0.05)\n",
    "axes[1].pie(target_counts.values, labels=['Did Not Subscribe', 'Subscribed'], \n",
    "            autopct='%1.1f%%', colors=colors, startangle=90, explode=explode,\n",
    "            textprops={'fontweight': 'bold', 'fontsize': 10}, shadow=True)\n",
    "axes[1].set_title('Class Distribution\\n(Percentage)', fontweight='bold', fontsize=12)\n",
    "\n",
    "axes[2].barh(['No', 'Yes'], [target_counts['no'], target_counts['yes']], \n",
    "             color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "axes[2].set_xlabel('Number of Clients (Log Scale)', fontweight='bold')\n",
    "axes[2].set_title('Class Imbalance\\n(Logarithmic View)', fontweight='bold', fontsize=12)\n",
    "axes[2].set_xscale('log')\n",
    "axes[2].grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.suptitle('Target Variable Analysis: Subscription to Term Deposit', \n",
    "             fontweight='bold', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575410b3",
   "metadata": {},
   "source": [
    "## 2.4 Numeric Features Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61801b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NUMERIC FEATURES ANALYSIS\")\n",
    "\n",
    "\n",
    "numeric_cols = ['age', 'balance', 'day', 'duration', 'campaign', 'pdays', 'previous']\n",
    "\n",
    "print(f\"\\nNumeric Features: {len(numeric_cols)}\")\n",
    "print(f\"Features: {', '.join(numeric_cols)}\")\n",
    "\n",
    "print(\"\\nDescriptive Statistics:\")\n",
    "desc_stats = df_bank[numeric_cols].describe().T\n",
    "desc_stats['skewness'] = df_bank[numeric_cols].skew()\n",
    "desc_stats['kurtosis'] = df_bank[numeric_cols].kurtosis()\n",
    "display(desc_stats)\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 12))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    df_bank[col].hist(bins=50, ax=ax, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "    df_bank[col].plot(kind='kde', ax=ax2, color='red', linewidth=2)\n",
    "    ax2.set_ylabel('Density', fontsize=9)\n",
    "    ax2.grid(False)\n",
    "    \n",
    "    mean_val = df_bank[col].mean()\n",
    "    median_val = df_bank[col].median()\n",
    "    ax.axvline(mean_val, color='green', linestyle='--', linewidth=2, \n",
    "               label=f'Mean: {mean_val:.1f}')\n",
    "    ax.axvline(median_val, color='orange', linestyle='--', linewidth=2, \n",
    "               label=f'Median: {median_val:.1f}')\n",
    "    \n",
    "    ax.set_title(f'{col.upper()}', fontweight='bold', fontsize=11)\n",
    "    ax.set_xlabel(col, fontsize=9)\n",
    "    ax.set_ylabel('Frequency', fontsize=9)\n",
    "    ax.legend(fontsize=7, loc='upper right')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "for i in range(len(numeric_cols), len(axes)):\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Distribution of Numeric Features with KDE Overlay', \n",
    "             fontweight='bold', fontsize=14, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Observations:\")\n",
    "print(\"  • AGE: Relatively normal distribution, mean 40.9 years\")\n",
    "print(\"  • BALANCE: Heavily right-skewed, large outliers (max €102,127)\")\n",
    "print(\"  • DAY: Fairly uniform across month days\")\n",
    "print(\"  • DURATION: Right-skewed, mean 258 seconds (~4.3 minutes)\")\n",
    "print(\"  • CAMPAIGN: Heavily right-skewed, most contacted 1-3 times\")\n",
    "print(\"  • PDAYS: Bimodal with huge spike at -1 (81.7% never contacted)\")\n",
    "print(\"  • PREVIOUS: Heavily right-skewed, most have 0 previous contacts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f9e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CATEGORICAL FEATURES ANALYSIS\")\n",
    "\n",
    "\n",
    "cat_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', \n",
    "            'contact', 'month', 'poutcome']\n",
    "\n",
    "print(f\"\\nCategorical Features: {len(cat_cols)}\")\n",
    "print(f\"Features: {', '.join(cat_cols)}\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 14))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(cat_cols):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    vc = df_bank[col].value_counts()\n",
    "    \n",
    "    colors_cat = plt.cm.Set3(range(len(vc)))\n",
    "    ax.barh(range(len(vc)), vc.values, color=colors_cat, \n",
    "            edgecolor='black', linewidth=1)\n",
    "    ax.set_yticks(range(len(vc)))\n",
    "    ax.set_yticklabels(vc.index)\n",
    "    ax.set_xlabel('Count', fontweight='bold')\n",
    "    ax.set_title(f'{col.upper()}\\n({len(vc)} categories)', \n",
    "                 fontweight='bold', fontsize=11)\n",
    "    ax.grid(alpha=0.3, axis='x')\n",
    "    \n",
    "    for j, v in enumerate(vc.values):\n",
    "        ax.text(v + 100, j, f'{v:,}', va='center', fontsize=8)\n",
    "\n",
    "plt.suptitle('Distribution of Categorical Features', \n",
    "             fontweight='bold', fontsize=14, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCategory Distribution Summary:\")\n",
    "for col in cat_cols:\n",
    "    unique_count = df_bank[col].nunique()\n",
    "    most_common = df_bank[col].value_counts().index[0]\n",
    "    most_common_pct = 100 * df_bank[col].value_counts().iloc[0] / len(df_bank)\n",
    "    print(f\"  • {col}: {unique_count} unique values, \"\n",
    "          f\"most common = '{most_common}' ({most_common_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "786553ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.5 Outlier Detection with Boxplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee5d5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"OUTLIER DETECTION\")\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(numeric_cols):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    Q1 = df_bank[col].quantile(0.25)\n",
    "    Q3 = df_bank[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = df_bank[(df_bank[col] < Q1 - 1.5*IQR) | (df_bank[col] > Q3 + 1.5*IQR)]\n",
    "    outlier_count = len(outliers)\n",
    "    outlier_pct = 100 * outlier_count / len(df_bank)\n",
    "    \n",
    "    bp = ax.boxplot(df_bank[col], vert=False, patch_artist=True,\n",
    "                    boxprops=dict(facecolor='lightcoral', alpha=0.7),\n",
    "                    medianprops=dict(color='red', linewidth=2),\n",
    "                    whiskerprops=dict(linewidth=1.5),\n",
    "                    capprops=dict(linewidth=1.5))\n",
    "    \n",
    "    ax.set_xlabel(col, fontweight='bold')\n",
    "    ax.set_title(f'{col.upper()}\\nOutliers: {outlier_count:,} ({outlier_pct:.1f}%)', \n",
    "                 fontweight='bold', fontsize=10)\n",
    "    ax.grid(alpha=0.3, axis='x')\n",
    "\n",
    "axes[-1].axis('off')\n",
    "\n",
    "plt.suptitle('Outlier Detection Using Boxplots', \n",
    "             fontweight='bold', fontsize=14, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nOutlier Summary:\")\n",
    "for col in numeric_cols:\n",
    "    Q1 = df_bank[col].quantile(0.25)\n",
    "    Q3 = df_bank[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    outliers = df_bank[(df_bank[col] < Q1 - 1.5*IQR) | (df_bank[col] > Q3 + 1.5*IQR)]\n",
    "    outlier_count = len(outliers)\n",
    "    if outlier_count > 0:\n",
    "        print(f\"  • {col}: {outlier_count:,} outliers ({100*outlier_count/len(df_bank):.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb86bc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.6 Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c3afa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CORRELATION ANALYSIS\")\n",
    "\n",
    "\n",
    "df_temp = df_bank.copy()\n",
    "df_temp['y_numeric'] = (df_temp['y'] == 'yes').astype(int)\n",
    "\n",
    "corr_matrix = df_temp[numeric_cols + ['y_numeric']].corr()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='RdBu_r', center=0, \n",
    "            square=True, ax=axes[0], cbar_kws={'shrink': 0.8},\n",
    "            linewidths=0.5, linecolor='gray')\n",
    "axes[0].set_title('Correlation Matrix\\n(Numeric Features + Target)', \n",
    "                  fontweight='bold', fontsize=12)\n",
    "\n",
    "target_corr = corr_matrix['y_numeric'].drop('y_numeric').sort_values(ascending=False)\n",
    "colors_corr = ['green' if x > 0 else 'red' for x in target_corr.values]\n",
    "\n",
    "axes[1].barh(range(len(target_corr)), target_corr.values, color=colors_corr, \n",
    "             alpha=0.7, edgecolor='black', linewidth=1.5)\n",
    "axes[1].set_yticks(range(len(target_corr)))\n",
    "axes[1].set_yticklabels(target_corr.index)\n",
    "axes[1].set_xlabel('Correlation Coefficient', fontweight='bold')\n",
    "axes[1].set_title('Feature Correlation with Target\\n(Subscription)', \n",
    "                  fontweight='bold', fontsize=12)\n",
    "axes[1].axvline(0, color='black', linewidth=1)\n",
    "axes[1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "for i, v in enumerate(target_corr.values):\n",
    "    axes[1].text(v + 0.01 if v > 0 else v - 0.01, i, f'{v:.3f}', \n",
    "                va='center', fontweight='bold', fontsize=9,\n",
    "                ha='left' if v > 0 else 'right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nTop Positive Correlations with Target:\")\n",
    "print(target_corr.head(3))\n",
    "print(\"\\nTop Negative Correlations with Target:\")\n",
    "print(target_corr.tail(3))\n",
    "\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"  • DURATION shows strongest positive correlation (0.41) but this is data leakage. Will be explained in more detail later.\")\n",
    "print(\"  • PDAYS shows moderate positive correlation (0.11)\")\n",
    "print(\"  • PREVIOUS shows weak positive correlation\")\n",
    "print(\"  • Most numeric features have weak correlation with target\")\n",
    "print(\"  • Limited multicollinearity among predictors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09046ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.7 Categorical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12ca257",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CATEGORICAL FEATURES ANALYSIS\")\n",
    "\n",
    "\n",
    "cat_cols = ['job', 'marital', 'education', 'default', 'housing', 'loan', \n",
    "            'contact', 'month', 'poutcome']\n",
    "\n",
    "print(f\"\\nCategorical Features: {len(cat_cols)}\")\n",
    "print(f\"Features: {', '.join(cat_cols)}\")\n",
    "\n",
    "fig, axes = plt.subplots(3, 3, figsize=(16, 14))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, col in enumerate(cat_cols):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    vc = df_bank[col].value_counts()\n",
    "    \n",
    "    colors_cat = plt.cm.Set3(range(len(vc)))\n",
    "    ax.barh(range(len(vc)), vc.values, color=colors_cat, \n",
    "            edgecolor='black', linewidth=1)\n",
    "    ax.set_yticks(range(len(vc)))\n",
    "    ax.set_yticklabels(vc.index)\n",
    "    ax.set_xlabel('Count', fontweight='bold')\n",
    "    ax.set_title(f'{col.upper()}\\n({len(vc)} categories)', \n",
    "                 fontweight='bold', fontsize=11)\n",
    "    ax.grid(alpha=0.3, axis='x')\n",
    "    \n",
    "    for j, v in enumerate(vc.values):\n",
    "        ax.text(v + 100, j, f'{v:,}', va='center', fontsize=8)\n",
    "\n",
    "plt.suptitle('Distribution of Categorical Features', \n",
    "             fontweight='bold', fontsize=14, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCategory Distribution Summary:\")\n",
    "for col in cat_cols:\n",
    "    unique_count = df_bank[col].nunique()\n",
    "    most_common = df_bank[col].value_counts().index[0]\n",
    "    most_common_pct = 100 * df_bank[col].value_counts().iloc[0] / len(df_bank)\n",
    "    print(f\"  • {col}: {unique_count} unique values, \"\n",
    "          f\"most common = '{most_common}' ({most_common_pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cdf5cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 2.8 Target vs Features Analysis (Subscription Patterns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27a889ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TARGET vs FEATURES ANALYSIS\")\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "age_bins = pd.cut(df_bank['age'], bins=[0, 30, 40, 50, 60, 100], \n",
    "                  labels=['<30', '30-40', '40-50', '50-60', '60+'])\n",
    "sub_by_age = df_bank.groupby(age_bins)['y'].apply(lambda x: (x=='yes').mean() * 100)\n",
    "\n",
    "axes[0].bar(range(len(sub_by_age)), sub_by_age.values, color='steelblue', \n",
    "            alpha=0.8, edgecolor='black')\n",
    "axes[0].set_xticks(range(len(sub_by_age)))\n",
    "axes[0].set_xticklabels(sub_by_age.index)\n",
    "axes[0].set_ylabel('Subscription Rate (%)', fontweight='bold')\n",
    "axes[0].set_xlabel('Age Group', fontweight='bold')\n",
    "axes[0].set_title('Subscription Rate by Age Group', fontweight='bold')\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "for i, v in enumerate(sub_by_age.values):\n",
    "    axes[0].text(i, v + 0.5, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "\n",
    "job_sub = df_bank.groupby('job')['y'].apply(lambda x: (x=='yes').mean() * 100).sort_values(ascending=False)\n",
    "axes[1].barh(range(len(job_sub)), job_sub.values, color='coral', alpha=0.8, edgecolor='black')\n",
    "axes[1].set_yticks(range(len(job_sub)))\n",
    "axes[1].set_yticklabels(job_sub.index)\n",
    "axes[1].set_xlabel('Subscription Rate (%)', fontweight='bold')\n",
    "axes[1].set_title('Subscription Rate by Job Type', fontweight='bold')\n",
    "axes[1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "edu_sub = df_bank.groupby('education')['y'].apply(lambda x: (x=='yes').mean() * 100).sort_values(ascending=False)\n",
    "axes[2].bar(range(len(edu_sub)), edu_sub.values, color='mediumseagreen', \n",
    "            alpha=0.8, edgecolor='black')\n",
    "axes[2].set_xticks(range(len(edu_sub)))\n",
    "axes[2].set_xticklabels(edu_sub.index, rotation=45, ha='right')\n",
    "axes[2].set_ylabel('Subscription Rate (%)', fontweight='bold')\n",
    "axes[2].set_title('Subscription Rate by Education', fontweight='bold')\n",
    "axes[2].grid(alpha=0.3, axis='y')\n",
    "\n",
    "marital_sub = df_bank.groupby('marital')['y'].apply(lambda x: (x=='yes').mean() * 100).sort_values(ascending=False)\n",
    "axes[3].bar(range(len(marital_sub)), marital_sub.values, color='purple', \n",
    "            alpha=0.8, edgecolor='black')\n",
    "axes[3].set_xticks(range(len(marital_sub)))\n",
    "axes[3].set_xticklabels(marital_sub.index)\n",
    "axes[3].set_ylabel('Subscription Rate (%)', fontweight='bold')\n",
    "axes[3].set_title('Subscription Rate by Marital Status', fontweight='bold')\n",
    "axes[3].grid(alpha=0.3, axis='y')\n",
    "\n",
    "pout_sub = df_bank.groupby('poutcome')['y'].apply(lambda x: (x=='yes').mean() * 100).sort_values(ascending=False)\n",
    "axes[4].bar(range(len(pout_sub)), pout_sub.values, color='teal', \n",
    "            alpha=0.8, edgecolor='black')\n",
    "axes[4].set_xticks(range(len(pout_sub)))\n",
    "axes[4].set_xticklabels(pout_sub.index, rotation=45, ha='right')\n",
    "axes[4].set_ylabel('Subscription Rate (%)', fontweight='bold')\n",
    "axes[4].set_title('Subscription Rate by Previous Outcome', fontweight='bold')\n",
    "axes[4].grid(alpha=0.3, axis='y')\n",
    "\n",
    "month_order = ['jan', 'feb', 'mar', 'apr', 'may', 'jun', 'jul', 'aug', 'sep', 'oct', 'nov', 'dec']\n",
    "month_sub = df_bank.groupby('month')['y'].apply(lambda x: (x=='yes').mean() * 100)\n",
    "month_sub = month_sub.reindex([m for m in month_order if m in month_sub.index])\n",
    "axes[5].plot(range(len(month_sub)), month_sub.values, marker='o', linewidth=2, \n",
    "             markersize=8, color='orange')\n",
    "axes[5].set_xticks(range(len(month_sub)))\n",
    "axes[5].set_xticklabels(month_sub.index, rotation=45, ha='right')\n",
    "axes[5].set_ylabel('Subscription Rate (%)', fontweight='bold')\n",
    "axes[5].set_title('Subscription Rate by Contact Month', fontweight='bold')\n",
    "axes[5].grid(alpha=0.3)\n",
    "axes[5].axhline(df_bank['y'].apply(lambda x: 1 if x=='yes' else 0).mean() * 100, \n",
    "                color='red', linestyle='--', label='Overall Avg')\n",
    "axes[5].legend()\n",
    "\n",
    "plt.suptitle('Subscription Patterns Across Different Features', \n",
    "             fontweight='bold', fontsize=14, y=0.995)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Subscription Patterns:\")\n",
    "print(f\"  • Age: Older customers (60+) have higher subscription rate\")\n",
    "print(f\"  • Job: Students and retired show highest rates\")\n",
    "print(f\"  • Education: Tertiary education shows highest rate\")\n",
    "print(f\"  • Marital: Single customers subscribe more\")\n",
    "print(f\"  • Previous Outcome: 'success' dramatically increases likelihood (65%+)\")\n",
    "print(f\"  • Month: March, September, October, December show higher rates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d924cdb9",
   "metadata": {},
   "source": [
    "## 2.9 EDA Summary\n",
    "\n",
    "**Key Findings from Exploratory Data Analysis:**\n",
    "\n",
    "1. **Data Quality:**\n",
    "   - Clean dataset with no missing values or duplicates\n",
    "   - Some 'unknown' categorical values present but valid\n",
    "\n",
    "2. **Target Variable:**\n",
    "   - Severe class imbalance (88.3% no, 11.7% yes)\n",
    "   - Reflects realistic campaign success rates\n",
    "\n",
    "3. **Numeric Features:**\n",
    "   - Age: Normal distribution around 40 years\n",
    "   - Balance: Heavy right skew with large outliers (needs log transformation)\n",
    "   - Duration: Right-skewed (data leakage issue - must remove)\n",
    "   - Campaign: Most clients contacted 1-3 times\n",
    "   - Pdays: 81.7% never previously contacted (-1 value)\n",
    "   - Previous: Heavily concentrated at 0\n",
    "\n",
    "4. **Outliers:**\n",
    "   - Balance: 6.4% outliers\n",
    "   - Campaign: 2.7% outliers\n",
    "   - Previous: 1.8% outliers\n",
    "   - Will need addressing in preprocessing\n",
    "\n",
    "5. **Correlations:**\n",
    "   - Duration shows strongest correlation (0.41) but is data leakage\n",
    "   - Other features show weak to moderate correlations\n",
    "   - Limited multicollinearity\n",
    "\n",
    "6. **Categorical Patterns:**\n",
    "   - Job: Blue-collar most common (21.5%)\n",
    "   - Education: Secondary most common (51.3%)\n",
    "   - Marital: Married most common (60.2%)\n",
    "   - Contact: Cellular most common (64.8%)\n",
    "   - Poutcome: Unknown dominates (81.8%)\n",
    "\n",
    "7. **Subscription Patterns:**\n",
    "   - Students and retirees subscribe more\n",
    "   - Single status increases subscription\n",
    "   - Previous success strongly predicts future success\n",
    "   - Seasonal effects evident (March, September peaks)\n",
    "\n",
    "**Implications for Clustering:**\n",
    "- High dimensionality after one-hot encoding (will need PCA)\n",
    "- Skewed features require transformation\n",
    "- Data leakage feature (duration) must be removed\n",
    "- Special handling needed for pdays (-1 value)\n",
    "- Outliers can be removed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19692757",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "def preprocess_bank_data(df):\n",
    "    \"\"\"    \n",
    "    Steps:\n",
    "    1. Remove target variable (unsupervised learning)\n",
    "    2. Remove duration (data leakage - only known after call)\n",
    "    3. Handle pdays special value (-1 = never contacted)\n",
    "    4. Transform skewed features (balance, campaign, previous)\n",
    "    5. Engineer binary indicators\n",
    "    6. One-hot encode categorical variables\n",
    "    7. Remove outliers using z-score\n",
    "    8. Standardize all numeric features\n",
    "    \"\"\"\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    \n",
    "    \n",
    "    print(\"PREPROCESSING PIPELINE\")\n",
    "    \n",
    "    \n",
    "    print(\"\\n1. Removing target variable 'y' (unsupervised learning)\")\n",
    "    if 'y' in df_processed.columns:\n",
    "        df_processed = df_processed.drop('y', axis=1)\n",
    "        print(\"Removed 'y' column\")\n",
    "    \n",
    "    print(\"\\n2. Removing 'duration' (data leakage issue)\")\n",
    "    if 'duration' in df_processed.columns:\n",
    "        df_processed = df_processed.drop('duration', axis=1)\n",
    "        print(\"Removed 'duration' column\")\n",
    "        print(\"Reason: Duration only known after call completion\")\n",
    "    \n",
    "    print(\"\\n3. Engineering features from 'pdays'\")\n",
    "    if 'pdays' in df_processed.columns:\n",
    "        never_contacted = (df_processed['pdays'] == -1).sum()\n",
    "        print(f\"   • pdays = -1 (never contacted): {never_contacted:,} ({100*never_contacted/len(df_processed):.1f}%)\")\n",
    "        \n",
    "        df_processed['was_previously_contacted'] = (df_processed['pdays'] > 0).astype(int)\n",
    "        \n",
    "        df_processed['days_since_last_contact'] = df_processed['pdays'].clip(lower=0)\n",
    "        \n",
    "        df_processed['recent_contact'] = (df_processed['pdays'].between(1, 30)).astype(int)\n",
    "        \n",
    "        df_processed = df_processed.drop('pdays', axis=1)\n",
    "        print(\"Created: was_previously_contacted, days_since_last_contact, recent_contact\")\n",
    "    \n",
    "    print(\"\\n4. Transforming skewed features\")\n",
    "    \n",
    "    if 'balance' in df_processed.columns:\n",
    "        print(f\"   • balance: range [{df_processed['balance'].min():,.0f}, {df_processed['balance'].max():,.0f}]\")\n",
    "        df_processed['balance_log'] = np.sign(df_processed['balance']) * np.log1p(np.abs(df_processed['balance']))\n",
    "        df_processed = df_processed.drop('balance', axis=1)\n",
    "        print(\"Applied signed log1p transformation to balance\")\n",
    "    \n",
    "    if 'campaign' in df_processed.columns:\n",
    "        df_processed['campaign_log'] = np.log1p(df_processed['campaign'])\n",
    "        df_processed['high_campaign_intensity'] = (df_processed['campaign'] > 3).astype(int)\n",
    "        df_processed = df_processed.drop('campaign', axis=1)\n",
    "        print(\"Log-transformed campaign, created high_campaign_intensity indicator\")\n",
    "    \n",
    "    if 'previous' in df_processed.columns:\n",
    "        df_processed['has_previous_contacts'] = (df_processed['previous'] > 0).astype(int)\n",
    "        df_processed['previous_capped'] = df_processed['previous'].clip(upper=10)\n",
    "        df_processed = df_processed.drop('previous', axis=1)\n",
    "        print(\"Created has_previous_contacts, capped previous at 10\")\n",
    "    \n",
    "    print(\"\\n5. Creating age group indicators\")\n",
    "    if 'age' in df_processed.columns:\n",
    "        df_processed['age_young'] = (df_processed['age'] < 30).astype(int)\n",
    "        df_processed['age_senior'] = (df_processed['age'] >= 60).astype(int)\n",
    "        df_processed['age_prime'] = (df_processed['age'].between(30, 50)).astype(int)\n",
    "        print(\"Created: age_young, age_senior, age_prime\")\n",
    "    \n",
    "    print(\"\\n6. Creating day-of-month patterns\")\n",
    "    if 'day' in df_processed.columns:\n",
    "        df_processed['day_first_week'] = (df_processed['day'] <= 7).astype(int)\n",
    "        df_processed['day_last_week'] = (df_processed['day'] >= 24).astype(int)\n",
    "        print(\"Created: day_first_week, day_last_week\")\n",
    "    \n",
    "    print(\"\\n7. One-hot encoding categorical variables\")\n",
    "    categorical_features = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "    \n",
    "    if categorical_features:\n",
    "        print(f\"Categorical features: {categorical_features}\")\n",
    "        df_processed = pd.get_dummies(df_processed, columns=categorical_features, \n",
    "                                       drop_first=True, dtype=int)\n",
    "        print(f\"   One-hot encoded {len(categorical_features)} categorical features\")\n",
    "    \n",
    "    print(f\"\\n   Shape after encoding: {df_processed.shape}\")\n",
    "    \n",
    "    print(\"\\n8. Removing outliers (z-score > 3.5)\")\n",
    "    initial_size = len(df_processed)\n",
    "    \n",
    "    outlier_cols = ['campaign_log', 'days_since_last_contact', 'previous_capped', 'balance_log']\n",
    "    outliers_removed = 0\n",
    "    \n",
    "    for col in outlier_cols:\n",
    "        if col in df_processed.columns:\n",
    "            z_scores = np.abs(stats.zscore(df_processed[col]))\n",
    "            mask = z_scores < 3.5\n",
    "            removed = (~mask).sum()\n",
    "            df_processed = df_processed[mask]\n",
    "            outliers_removed += removed\n",
    "            if removed > 0:\n",
    "                print(f\"   • {col}: removed {removed:,} outliers\")\n",
    "    \n",
    "    print(f\"\\n   Total outliers removed: {outliers_removed:,} ({100*outliers_removed/initial_size:.2f}%)\")\n",
    "    print(f\"   Remaining samples: {len(df_processed):,}\")\n",
    "    \n",
    "    print(\"\\n9. Standardizing all features (z-score normalization)\")\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    numeric_cols = df_processed.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    df_processed[numeric_cols] = scaler.fit_transform(df_processed[numeric_cols])\n",
    "    \n",
    "    numeric_features = numeric_cols\n",
    "    categorical_features = []  \n",
    "\n",
    "    df_scaled = df_processed[numeric_features]\n",
    "    df_encoded = df_processed[categorical_features] if categorical_features else pd.DataFrame()\n",
    "\n",
    "    print(\"\\nData ready for clustering:\")\n",
    "    print(f\"  • Processed shape: {df_processed.shape}\")\n",
    "    print(f\"  • Numeric features: {len(numeric_features)}\")\n",
    "    print(f\"  • Encoded categorical: {len(categorical_features)}\")\n",
    "\n",
    "    return df_processed, df_scaled, df_encoded\n",
    "\n",
    "\n",
    "X_processed, df_scaled, df_encoded = preprocess_bank_data(df_bank)\n",
    "X_array = X_processed.to_numpy()\n",
    "\n",
    "print(f\"\\nReady for clustering:\")\n",
    "print(f\"  • Data type: {X_array.dtype}\")\n",
    "print(f\"  • Shape: {X_array.shape}\")\n",
    "print(f\"  • No NaN values: {not np.isnan(X_array).any()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e633a263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "\n",
    "def get_labels_from_clusters(kmeans_model, X):\n",
    "    \"\"\"\n",
    "    returns cluster labels for each point given trained KMeansClustering model\n",
    "    \"\"\"\n",
    "    centroids = kmeans_model.centroids\n",
    "    labels = np.zeros(len(X), dtype=int)\n",
    "    for i, x in enumerate(X):\n",
    "        distances = np.linalg.norm(x - centroids, axis=1)\n",
    "        labels[i] = np.argmin(distances)\n",
    "    return labels\n",
    "\n",
    "\n",
    "def compute_inertia(X, labels, centroids):\n",
    "    \"\"\"Compute inertia (within-cluster sum of squares)\"\"\"\n",
    "    inertia = 0\n",
    "    for i in range(len(centroids)):\n",
    "        cluster_points = X[labels == i]\n",
    "        if len(cluster_points) > 0:\n",
    "            inertia += np.sum((cluster_points - centroids[i]) ** 2)\n",
    "    return inertia\n",
    "\n",
    "\n",
    "\n",
    "class KMeansClustering:\n",
    "    def __init__(self, k, epsilon=1e-4, max_iter=500):\n",
    "        self.k = k\n",
    "        self.epsilon = epsilon\n",
    "        self.max_iter = max_iter\n",
    "        self.centroids = None\n",
    "        self.labels_ = None\n",
    "\n",
    "    def initialize_centroids(self, X):\n",
    "        \"\"\"randomly pick k points as initial centroids\"\"\"\n",
    "        indices = np.random.choice(X.shape[0], self.k, replace=False)\n",
    "        self.centroids = X[indices]\n",
    "\n",
    "    def assign_clusters(self, X):\n",
    "        \"\"\"assign each point to the nearest centroid (vectorized)\"\"\"\n",
    "        distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2)\n",
    "        return np.argmin(distances, axis=1)\n",
    "\n",
    "    def update_centroids(self, X, labels):\n",
    "        \"\"\"update centroids as mean of assigned points\"\"\"\n",
    "        new_centroids = np.zeros_like(self.centroids)\n",
    "        for i in range(self.k):\n",
    "            points = X[labels == i]\n",
    "            if len(points) > 0:\n",
    "                new_centroids[i] = points.mean(axis=0)\n",
    "            else:\n",
    "                new_centroids[i] = self.centroids[i]\n",
    "        return new_centroids\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.initialize_centroids(X)\n",
    "        for iteration in range(self.max_iter):\n",
    "            labels = self.assign_clusters(X)\n",
    "            new_centroids = self.update_centroids(X, labels)\n",
    "            shift = np.linalg.norm(self.centroids - new_centroids)\n",
    "            if shift < self.epsilon:\n",
    "                print(f\"converged at iteration {iteration+1}\")\n",
    "                break\n",
    "            self.centroids = new_centroids\n",
    "        self.labels_ = labels\n",
    "        return self\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"K-MEANS CLUSTERING - PARAMETER TUNING\")\n",
    "\n",
    "\n",
    "K_range = range(2, 3)\n",
    "inertias = []\n",
    "silhouette_scores = []\n",
    "\n",
    "print(\"\\nTesting different values of k...\\n\")\n",
    "\n",
    "for k in K_range:\n",
    "    print(f\"k = {k}:\")\n",
    "    kmeans = KMeansClustering(k=k, epsilon=1e-9, max_iter=2000)\n",
    "    model = kmeans.fit(X_array)\n",
    "    labels = model.labels_\n",
    "    inertia = compute_inertia(X_array, labels, model.centroids)\n",
    "    sil_score = silhouette_score(X_array, labels)\n",
    "    \n",
    "    inertias.append(inertia)\n",
    "    silhouette_scores.append(sil_score)\n",
    "    \n",
    "    print(f\"  Inertia: {inertia:.2f}\")\n",
    "    print(f\"  Silhouette Score: {sil_score:.4f}\\n\")\n",
    "\n",
    "best_k = K_range[np.argmax(silhouette_scores)]\n",
    "best_silhouette = max(silhouette_scores)\n",
    "\n",
    "\n",
    "print(f\"OPTIMAL k = {best_k} (Silhouette Score: {best_silhouette:.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"\\nFitting final K-Means model with k={best_k}...\")\n",
    "kmeans_final = KMeansClustering(k=best_k, epsilon=1e-9, max_iter=2000).fit(X_array)\n",
    "labels_final = kmeans_final.labels_\n",
    "\n",
    "silhouette = silhouette_score(X_array, labels_final)\n",
    "db_index = davies_bouldin_score(X_array, labels_final)\n",
    "\n",
    "print(f\"\\nFinal K-Means Results:\")\n",
    "print(f\"  Number of clusters: {best_k}\")\n",
    "print(f\"  Silhouette Score: {silhouette:.4f}\")\n",
    "print(f\"  Davies–Bouldin Index: {db_index:.4f}\")\n",
    "\n",
    "print(f\"\\nCluster Sizes:\")\n",
    "cluster_counts = pd.Series(labels_final).value_counts().sort_index()\n",
    "for cluster_id, count in cluster_counts.items():\n",
    "    print(f\"  Cluster {cluster_id}: {count:,} samples ({100*count/len(labels_final):.1f}%)\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_array)\n",
    "\n",
    "plt.figure(figsize=(10, 7))\n",
    "scatter = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], c=labels_final, cmap='tab10', s=30, alpha=0.6)\n",
    "plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%})', fontweight='bold')\n",
    "plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%})', fontweight='bold')\n",
    "plt.title(f'K-Means Clustering (k={best_k}) - 2D PCA Projection\\nSilhouette: {silhouette:.4f}, DB Index: {db_index:.4f}',\n",
    "          fontweight='bold')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee22eca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "\n",
    "silhouette = silhouette_score(X_array, labels_final)\n",
    "db_index = davies_bouldin_score(X_array, labels_final)\n",
    "\n",
    "print(f\"\\nFinal Evaluation Metrics:\")\n",
    "print(f\"  Silhouette Score: {silhouette:.4f}\")\n",
    "print(f\"  Davies–Bouldin Index: {db_index:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5205f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DIMENSIONALITY REDUCTION WITH PCA\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nRationale:\")\n",
    "print(\"  • Current dataset has 50 features after preprocessing\")\n",
    "print(\"  • High-dimensional data suffers from 'curse of dimensionality'\")\n",
    "print(\"  • PCA reduces noise and computational complexity\")\n",
    "print(\"  • Improves clustering by focusing on variance-explaining components\")\n",
    "print(\"  • Makes visualization more interpretable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6a66d7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "pca_full = PCA()\n",
    "pca_full.fit(X_array)\n",
    "\n",
    "cumulative_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "\n",
    "print(f\"\\n PCA Variance Analysis:\")\n",
    "print(f\"  • Total features: {X_array.shape[1]}\")\n",
    "print(f\"  • Components for 95% variance: {n_components_95}\")\n",
    "print(f\"  • Components for 90% variance: {np.argmax(cumulative_variance >= 0.90) + 1}\")\n",
    "print(f\"  • Dimensionality reduction: {X_array.shape[1]} → {n_components_95} ({100*(1-n_components_95/X_array.shape[1]):.1f}% reduction)\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].bar(range(1, min(21, len(pca_full.explained_variance_ratio_)+1)), \n",
    "            pca_full.explained_variance_ratio_[:20], \n",
    "            color='steelblue', alpha=0.7, edgecolor='black')\n",
    "axes[0].set_xlabel('Principal Component', fontweight='bold')\n",
    "axes[0].set_ylabel('Explained Variance Ratio', fontweight='bold')\n",
    "axes[0].set_title('Scree Plot: Variance per Component', fontweight='bold')\n",
    "axes[0].grid(alpha=0.3, axis='y')\n",
    "\n",
    "axes[1].plot(range(1, len(cumulative_variance)+1), cumulative_variance, \n",
    "             marker='o', linewidth=2, markersize=4, color='darkgreen')\n",
    "axes[1].axhline(0.95, color='red', linestyle='--', linewidth=2, label='95% threshold')\n",
    "axes[1].axvline(n_components_95, color='orange', linestyle='--', linewidth=2, \n",
    "                label=f'{n_components_95} components')\n",
    "axes[1].set_xlabel('Number of Components', fontweight='bold')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance', fontweight='bold')\n",
    "axes[1].set_title('Cumulative Variance Explained', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "pca_reducer = PCA(n_components=n_components_95)\n",
    "X_pca = pca_reducer.fit_transform(X_array)\n",
    "\n",
    "print(f\"\\n PCA transformation complete:\")\n",
    "print(f\"  • Reduced shape: {X_pca.shape}\")\n",
    "print(f\"  • Variance retained: {cumulative_variance[n_components_95-1]:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf7bb34",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(\"K-MEANS CLUSTERING ON PCA-REDUCED DATA\")\n",
    "\n",
    "K_range_pca = range(2, 5)\n",
    "inertias_pca = []\n",
    "silhouette_scores_pca = []\n",
    "\n",
    "print(\"\\n Testing K-Means with different k values on PCA data...\")\n",
    "\n",
    "for k in K_range_pca:\n",
    "    kmeans_pca = KMeansClustering(k=k, epsilon=1e-9, max_iter=2000)\n",
    "    model_pca = kmeans_pca.fit(X_pca)\n",
    "    labels_pca = model_pca.labels_\n",
    "    \n",
    "    inertia_pca = compute_inertia(X_pca, labels_pca, model_pca.centroids)\n",
    "    sil_score_pca = silhouette_score(X_pca, labels_pca)\n",
    "    \n",
    "    inertias_pca.append(inertia_pca)\n",
    "    silhouette_scores_pca.append(sil_score_pca)\n",
    "    \n",
    "    print(f\"  k={k:2d} | Inertia: {inertia_pca:>12,.2f} | Silhouette: {sil_score_pca:.4f}\")\n",
    "\n",
    "best_k_pca = K_range_pca[np.argmax(silhouette_scores_pca)]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].plot(K_range_pca, inertias_pca, marker='o', linewidth=2, markersize=8, color='navy')\n",
    "axes[0].set_xlabel('Number of Clusters (k)', fontweight='bold')\n",
    "axes[0].set_ylabel('Inertia (WCSS)', fontweight='bold')\n",
    "axes[0].set_title('Elbow Method - PCA Data', fontweight='bold')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(K_range_pca, silhouette_scores_pca, marker='s', linewidth=2, \n",
    "             markersize=8, color='darkred')\n",
    "axes[1].axvline(best_k_pca, color='green', linestyle='--', linewidth=2, \n",
    "                label=f'Best k={best_k_pca}')\n",
    "axes[1].set_xlabel('Number of Clusters (k)', fontweight='bold')\n",
    "axes[1].set_ylabel('Silhouette Score', fontweight='bold')\n",
    "axes[1].set_title('Silhouette Score vs k - PCA Data', fontweight='bold')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n Optimal k for PCA data: {best_k_pca} (Silhouette: {max(silhouette_scores_pca):.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b8747a",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mThe kernel died. Error: ... View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "print(f\"FITTING FINAL K-MEANS MODEL (k={best_k_pca}) ON PCA DATA\")\n",
    "\n",
    "kmeans_pca_final = KMeansClustering(k=best_k_pca, epsilon=1e-9, max_iter=2000)\n",
    "kmeans_pca_final.fit(X_pca)\n",
    "labels_pca_final = kmeans_pca_final.labels_\n",
    "\n",
    "silhouette_pca = silhouette_score(X_pca, labels_pca_final)\n",
    "db_index_pca = davies_bouldin_score(X_pca, labels_pca_final)\n",
    "\n",
    "print(f\"\\n Final K-Means Performance (PCA-Reduced Data):\")\n",
    "print(f\"  • Number of clusters: {best_k_pca}\")\n",
    "print(f\"  • Silhouette Score: {silhouette_pca:.4f}\")\n",
    "print(f\"  • Davies-Bouldin Index: {db_index_pca:.4f}\")\n",
    "\n",
    "print(f\"\\n Cluster Distribution:\")\n",
    "cluster_counts_pca = pd.Series(labels_pca_final).value_counts().sort_index()\n",
    "for cluster_id, count in cluster_counts_pca.items():\n",
    "    pct = 100 * count / len(labels_pca_final)\n",
    "    print(f\"  Cluster {cluster_id}: {count:>6,} samples ({pct:>5.1f}%)\")\n",
    "\n",
    "pca_2d = PCA(n_components=2)\n",
    "X_pca_2d = pca_2d.fit_transform(X_pca)\n",
    "\n",
    "plt.figure(figsize=(12, 7))\n",
    "scatter = plt.scatter(X_pca_2d[:, 0], X_pca_2d[:, 1], \n",
    "                     c=labels_pca_final, cmap='viridis', \n",
    "                     s=20, alpha=0.6, edgecolors='k', linewidth=0.3)\n",
    "plt.xlabel(f'PC1 ({pca_2d.explained_variance_ratio_[0]:.1%} variance)', fontweight='bold', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca_2d.explained_variance_ratio_[1]:.1%} variance)', fontweight='bold', fontsize=12)\n",
    "plt.title(f'K-Means Clustering on PCA Data (k={best_k_pca})\\n' + \n",
    "          f'Silhouette: {silhouette_pca:.4f} | DB Index: {db_index_pca:.4f}',\n",
    "          fontweight='bold', fontsize=13)\n",
    "plt.colorbar(scatter, label='Cluster ID')\n",
    "plt.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff02cb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"COMPARISON: K-MEANS ON ORIGINAL vs PCA-REDUCED DATA\")\n",
    "\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Metric': ['Dataset Dimensions', 'Optimal k', 'Silhouette Score', 'Davies-Bouldin Index'],\n",
    "    'Original Data': [\n",
    "        f'{X_array.shape[1]} features',\n",
    "        best_k,\n",
    "        f'{silhouette:.4f}',\n",
    "        f'{db_index:.4f}'\n",
    "    ],\n",
    "    'PCA-Reduced Data': [\n",
    "        f'{X_pca.shape[1]} components',\n",
    "        best_k_pca,\n",
    "        f'{silhouette_pca:.4f}',\n",
    "        f'{db_index_pca:.4f}'\n",
    "    ]\n",
    "})\n",
    "\n",
    "display(comparison_df)\n",
    "\n",
    "print(\"\\n Key Observations:\")\n",
    "print(f\"  • Dimensionality: Reduced from {X_array.shape[1]} to {X_pca.shape[1]} ({100*(1-X_pca.shape[1]/X_array.shape[1]):.1f}% reduction)\")\n",
    "print(f\"  • Silhouette improvement: {((silhouette_pca - silhouette)/silhouette*100):+.1f}%\")\n",
    "print(f\"  • DB Index improvement: {((db_index - db_index_pca)/db_index*100):+.1f}% (lower is better)\")\n",
    "\n",
    "if silhouette_pca > silhouette:\n",
    "    print(f\"\\n PCA improves clustering quality by reducing noise and emphasizing variance\")\n",
    "else:\n",
    "    print(f\"\\n Original data performs slightly better, suggesting information loss in PCA\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
